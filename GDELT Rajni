#Import data
GDELT <- read.csv("C:\\Users\\sandhr\\Downloads\\GDELT_gkg_f8_package1.csv")

#number of rows and columns
ncol(GDELT)
nrow(GDELT)

#column names
colnames(GDELT)

#content of columns
str(GDELT)

#range of the date variable
GDELT$dateTimeDocument <- sort(GDELT$dateTimeDocument)
  #earliest entry
  entryfirst <- GDELT[1,]  
  #last entry
  entrylast <- GDELT[nrow(GDELT),]
  
#transform format of dateTimeDocument column
dates <- substr(GDELT$dateTimeDocument, 1, 10)
times <- substr(GDELT$dateTimeDocument, 12, 19)
GDELT$dateTime <- paste(dates, times)
GDELT$dateTime <- strptime(GDELT$dateTime,format='%Y-%m-%d %H:%M:%S')

#how many articles in each month (There's probably a better way of doing this)
months <- substr(GDELT$dateTimeDocument, 1, 7)
monthsdf <- as.data.frame(months)
library(dplyr)
a <- count(monthsdf, months)



#getting the first value out of tone to see the positivity or negativity of the article
class(GDELT$tone)
#GDELT$tone_num <- as.numeric(GDELT$tone)


#GDELT$average <- substr(GDELT$tone, 1, 4)
#GDELT$average_num <- as.numeric(GDELT$tone)
#class(GDELT$average_num)
#GDELT$average_num[is.na(GDELT$average_num)] <- 0
#class(GDELT$average_num)
#GDELT$average_num <- sort(GDELT$average_num)
#GDELT <- GDELT[order(average_num)]

GDELT1 <- GDELT
#GDELT$new <- is.na(GDELT$average_num)
#GDELT$tone1 <- subset(GDELT, is.na(GDELT$average_num))

GDELT1$tone1 <- gsub("^(.*?),.*","\\1", GDELT1$tone)
GDELT1$tone1 <- as.numeric(GDELT1$tone1)
#class(GDELT1$tone1)

#getting the website name out of the url
bbc <- subset(GDELT1, grepl(pattern = "/BBC ", x = GDELT1$documentSource))
yahoo <- subset(GDELT1, grepl(pattern = ".yahoo.", x = GDELT1$documentSource))
GDELT2 <- GDELT1[!grepl("/BBC ", GDELT1$documentSource),]
GDELT2 <- GDELT2[!grepl(".yahoo.", GDELT2$documentSource),]

#remove the parts of the URL we won't need (as accurately as possible)
GDELT$Source <- sub(".*https://news.", "", GDELT$documentSource)
GDELT$Source <- sub(".*http://news.", "", GDELT$Source)
GDELT$Source <- sub(".*http://", "", GDELT$Source)
GDELT$Source <- sub(".*https://", "", GDELT$Source)
GDELT$Source <- sub(".*www2.", "", GDELT$Source)
GDELT$Source <- sub(".*www5.", "", GDELT$Source)
GDELT$Source <- sub(".*www3.", "", GDELT$Source)
GDELT$Source <- sub(".*www10.", "", GDELT$Source)
GDELT$Source <- sub(".*www.", "", GDELT$Source)
GDELT$Source <- sub('\\..*', '', GDELT$Source)

#only interested in the Source and the tone of the articles
GDELT$Source <- as.factor(GDELT$Source)
articlecount <- GDELT[c("Source", "tone1")]
articlecount <- articlecount[order(articlecount$Source),]

  #add the tones for yahoo and bbc back in 
  yahoo$Source <- "yahoo"
  yahoo <- yahoo[c("Source", "tone1")]
  bbc$Source <- "bbc"
  bbc <- bbc[c("Source", "tone1")]
  articlecount <- rbind(articlecount, bbc, yahoo)

#count the number of articles for each Source
library(dplyr)
narticles <- count(articlecount, Source)
narticles <- narticles[order(narticles$n, decreasing=TRUE),]
top20 <- narticles[1:20,] #The top 20 websites that produce the most articles

#Get the average tone of each of these 20 websites
avgtone20 <- as.data.frame(matrix(nrow=20, ncol=3))
names(avgtone20) <- c("Source", "meantone", "sd")
avgtone20[,1] <- top20$Source
for (i in 1:20) {
  a <- subset(articlecount, grepl(pattern = paste0("^",top20$Source[i],"$"), x = articlecount$Source))
  avgtone20[i,2] <- mean(a$tone1)
  avgtone20[i,3] <- sd(a$tone1)
  }

#Take only the top 20 websites from articlecount so the tones can be plotted
tones20 <- data.frame()
for (i in 1:20) {
  a <- subset(articlecount, grepl(pattern = paste0("^",top20$Source[i],"$"), x = articlecount$Source))
  tones20 <- rbind(tones20, a)
}

#plot the data for the top 20 websites
library(ggplot2)
ggplot(avgtone20, aes(x = Source, y = meantone)) + geom_bar(stat = "identity") 

ggplot() + geom_point(data = tones20, aes(Source, tone1), colour = 'red', size = 3)

ggplot() + 
geom_point(data = avgtone20, aes(Source, meantone)) +
geom_errorbar(
  data = avgtone20,
  aes(Source, meantone, ymin = meantone - sd, ymax = meantone + sd),
  colour = 'red',
  width = 0.4
)



#Import data with a loop
GDELT <- data.frame()
for ( i in 1:7) {
  file <- read.csv(paste0("C:\\Users\\sandhr\\Downloads\\GDELT_gkg_f8_package",i ,".csv"))
  GDELT <- rbind(GDELT, file)
}

GDELT1 <- GDELT
GDELT <- GDELT1[1:1000,]


#CONTEXT
## how often are urls meaningful?

  #noticed that the meaningful part of the url usually contains a - or a _ so will remove any rows that dont contain this
meaningful <- GDELT[grepl("-", GDELT$documentSource) | grepl("_", GDELT$documentSource),]
  #from the dataset that had 463,324 rows, there are 381,293 meaningful urls

  #alternative way of doing the same thing
meaningful <- GDELT %>% filter(str_detect(documentSource, "_|-"))

##how many websites lead to errors?

##how often are there duplicate articles?

  #posted by the same news outlet several times

match <- function(GDELT){
  match  <- as.data.frame(matrix(ncol=2, nrow=nrow(GDELT)))
  names(match) <- c("url", "n")
    for (i in 1:nrow(GDELT)){
      url <- substr(GDELT$documentSource[i], 1, nchar(as.character(GDELT$documentSource[i])) - 2)
      number <- sum(grepl(url, GDELT$documentSource))
      match[i,1] <- url
      match[i,2] <- number
    }
  match1 <- unique(subset(match, match[,2] > 1))
  return(match1)
}

matches <- match(GDELT)

  #matches is a dataset that contains a column of the urls that are repeated and then a column showing how many times it is repeated

  #similar urls posted by several news outlets 
source_character <- as.character(GDELT$documentSource) 
split <- strsplit(source_character, "/")
split <- unlist(split)
decide <- as.data.frame(split)
decide <- as.data.frame(decide[grepl("-", decide$split) | grepl("_", decide$split),])
names(decide) <- c("content")

match1 <- function(decide){
  match  <- as.data.frame(matrix(ncol=2, nrow=nrow(decide)))
  names(match) <- c("content", "n")
  for (i in 1:nrow(decide)){
    content <- decide$content[i]
    number <- sum(grepl(content, decide$content))
    match[i,1] <- as.character(content)
    match[i,2] <- number
  }
  match1 <- unique(subset(match, match[,2] > 1))
  return(match1)
}

matches1 <- match1(decide)
  #matches1 is not a very tidy dataset but does highlight the stories that were posted more than once. 
  #it doesn't say by which news outlets though
