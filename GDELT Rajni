#Import data
GDELT <- read.csv("C:\\Users\\sandhr\\Downloads\\GDELT_gkg_f8_package1.csv")

#number of rows and columns
ncol(GDELT)
nrow(GDELT)

#column names
colnames(GDELT)

#content of columns
str(GDELT)

#range of the date variable
GDELT$dateTimeDocument <- sort(GDELT$dateTimeDocument)
  #earliest entry
  entryfirst <- GDELT[1,]  
  #last entry
  entrylast <- GDELT[nrow(GDELT),]
  
#transform format of dateTimeDocument column
dates <- substr(GDELT$dateTimeDocument, 1, 10)
times <- substr(GDELT$dateTimeDocument, 12, 19)
GDELT$dateTime <- paste(dates, times)
GDELT$dateTime <- strptime(GDELT$dateTime,format='%Y-%m-%d %H:%M:%S')

#how many articles in each month (There's probably a better way of doing this)
months <- substr(GDELT$dateTimeDocument, 1, 7)
monthsdf <- as.data.frame(months)
library(dplyr)
a <- count(monthsdf, months)



#getting the first value out of tone to see the positivity or negativity of the article
class(GDELT$tone)
#GDELT$tone_num <- as.numeric(GDELT$tone)


#GDELT$average <- substr(GDELT$tone, 1, 4)
#GDELT$average_num <- as.numeric(GDELT$tone)
#class(GDELT$average_num)
#GDELT$average_num[is.na(GDELT$average_num)] <- 0
#class(GDELT$average_num)
#GDELT$average_num <- sort(GDELT$average_num)
#GDELT <- GDELT[order(average_num)]

GDELT1 <- GDELT
#GDELT$new <- is.na(GDELT$average_num)
#GDELT$tone1 <- subset(GDELT, is.na(GDELT$average_num))

GDELT1$tone1 <- gsub("^(.*?),.*","\\1", GDELT1$tone)
GDELT1$tone1 <- as.numeric(GDELT1$tone1)
#class(GDELT1$tone1)

#getting the website name out of the url
bbc <- subset(GDELT1, grepl(pattern = "/BBC ", x = GDELT1$documentSource))
yahoo <- subset(GDELT1, grepl(pattern = ".yahoo.", x = GDELT1$documentSource))
GDELT2 <- GDELT1[!grepl("/BBC ", GDELT1$documentSource),]
GDELT2 <- GDELT2[!grepl(".yahoo.", GDELT2$documentSource),]

#remove the parts of the URL we won't need (as accurately as possible)
GDELT$Source <- sub(".*https://news.", "", GDELT$documentSource)
GDELT$Source <- sub(".*http://news.", "", GDELT$Source)
GDELT$Source <- sub(".*http://", "", GDELT$Source)
GDELT$Source <- sub(".*https://", "", GDELT$Source)
GDELT$Source <- sub(".*www2.", "", GDELT$Source)
GDELT$Source <- sub(".*www5.", "", GDELT$Source)
GDELT$Source <- sub(".*www3.", "", GDELT$Source)
GDELT$Source <- sub(".*www10.", "", GDELT$Source)
GDELT$Source <- sub(".*www.", "", GDELT$Source)
GDELT$Source <- sub('\\..*', '', GDELT$Source)

#only interested in the Source and the tone of the articles
GDELT$Source <- as.factor(GDELT$Source)
articlecount <- GDELT[c("Source", "tone1")]
articlecount <- articlecount[order(articlecount$Source),]

  #add the tones for yahoo and bbc back in 
  yahoo$Source <- "yahoo"
  yahoo <- yahoo[c("Source", "tone1")]
  bbc$Source <- "bbc"
  bbc <- bbc[c("Source", "tone1")]
  articlecount <- rbind(articlecount, bbc, yahoo)

#count the number of articles for each Source
library(dplyr)
narticles <- count(articlecount, Source)
narticles <- narticles[order(narticles$n, decreasing=TRUE),]
top20 <- narticles[1:20,] #The top 20 websites that produce the most articles

#Get the average tone of each of these 20 websites
avgtone20 <- as.data.frame(matrix(nrow=20, ncol=3))
names(avgtone20) <- c("Source", "meantone", "sd")
avgtone20[,1] <- top20$Source
for (i in 1:20) {
  a <- subset(articlecount, grepl(pattern = paste0("^",top20$Source[i],"$"), x = articlecount$Source))
  avgtone20[i,2] <- mean(a$tone1)
  avgtone20[i,3] <- sd(a$tone1)
  }

#Take only the top 20 websites from articlecount so the tones can be plotted
tones20 <- data.frame()
for (i in 1:20) {
  a <- subset(articlecount, grepl(pattern = paste0("^",top20$Source[i],"$"), x = articlecount$Source))
  tones20 <- rbind(tones20, a)
}

#plot the data for the top 20 websites
library(ggplot2)
ggplot(avgtone20, aes(x = Source, y = meantone)) + geom_bar(stat = "identity") 

ggplot() + geom_point(data = tones20, aes(Source, tone1), colour = 'red', size = 3)

ggplot() + 
geom_point(data = avgtone20, aes(Source, meantone)) +
geom_errorbar(
  data = avgtone20,
  aes(Source, meantone, ymin = meantone - sd, ymax = meantone + sd),
  colour = 'red',
  width = 0.4
)



#Import data with a loop
GDELT <- data.frame()
for ( i in 1:7) {
  file <- read.csv(paste0("C:\\Users\\sandhr\\Downloads\\GDELT_gkg_f8_package",i ,".csv"))
  GDELT <- rbind(GDELT, file)
}

GDELT1 <- GDELT
GDELT <- GDELT1[1:1000,]


#CONTEXT
## how often are urls meaningful?

  #noticed that the meaningful part of the url usually contains a - or a _ so will remove any rows that dont contain this
meaningful <- GDELT[grepl("-", GDELT$documentSource) | grepl("_", GDELT$documentSource),]
  #from the dataset that had 463,324 rows, there are 381,293 meaningful urls

  #alternative way of doing the same thing
library(dplyr, lib.loc="D:\\R\\Lib")
library(stringr, lib.loc="D:\\R\\Lib")
meaningful <- GDELT %>% filter(str_detect(documentSource, "_|-"))

##how many websites lead to errors?

##how often are there duplicate articles?

  #posted by the same news outlet several times

match <- function(GDELT){
  match  <- as.data.frame(matrix(ncol=2, nrow=nrow(GDELT)))
  names(match) <- c("url", "n")
    for (i in 1:nrow(GDELT)){
      url <- substr(GDELT$documentSource[i], 1, nchar(as.character(GDELT$documentSource[i])) - 10)
      number <- sum(grepl(url, GDELT$documentSource))
      match[i,1] <- url
      match[i,2] <- number
    }
  match1 <- unique(subset(match, match[,2] > 1))
  return(match1)
}

matches <- match(GDELT)
#matches is a dataset that contains a column of the urls that are repeated and then a column showing how many times it is repeated
#matches shows 47 observations out of 1000


#alternative method - this removes the last 10 characters from the url then finds duplicates
library(janitor, lib.loc="D:\\R\\Lib")
GDELT$url10 <- substr(GDELT$documentSource, 1, nchar(as.character(GDELT$documentSource)) - 10)
duplicates <- GDELT %>% get_dupes(url10)
duplicates <- duplicates[,1:2]
duplicates10 <- unique(duplicates)
#duplicates10 shows 11392 duplicates

#alternative method - this removes all the numbers from the urls then finds duplicates
library(NLP, lib.loc="D:\\R\\Lib")
library(tm, lib.loc="D:\\R\\Lib")
GDELT$urlc <- removeNumbers(as.character(GDELT$documentSource))
duplicates <- GDELT %>% get_dupes(urlc)
duplicates <- duplicates[,1:2]
duplicatesc <- unique(duplicates)
#duplicatescc also shows 8250 duplictaes...

duplicates <- GDELT %>% get_dupes(documentSource)
duplicates <- duplicates[,1:2]
duplicatesfull <- unique(duplicates)
#duplicatesfull shows 47 duplicates


  #similar urls posted by several news outlets 
source_character <- as.character(GDELT$documentSource) 
split <- strsplit(source_character, "/")
split <- unlist(split)
decide <- as.data.frame(split)
#this is a dataframe with a column containing a row for each section of a url separated by a /. so there are a lot of blank and
#unecessary rows.
#by taking only the rows that contain - or _ we are left with mostly the article topics
decide <- as.data.frame(decide[grepl("-", decide$split) | grepl("_", decide$split),])
names(decide) <- c("content")

library(janitor, lib.loc="D:\\R\\Lib")
library(dplyr, lib.loc="D:\\R\\Lib")
duplicates <- decide %>% get_dupes(content)
duplicates <- duplicates[,1:2]
duplicates <- unique(duplicates)
#37331 repeated rows

#majority of rows are not article titles as all sections of the url are taken. 
#assuming the most article titles have more than 30 characters, remove any rows that have less than 30 characters
#a few article titles are lost by doing this but most rows under 30 characters dont appear to be article titles
duplicates <- duplicates[nchar(as.character(duplicates$content)) > 30,]
duplicates <- duplicates[order(duplicates$dupe_count, decreasing=TRUE),]
#28276 repeated rows

#compare themes and themesV2

#bring in the list of the 284 possible themes that this column uses
listthemes <- read.table("H:\\My Documents\\work\\R learning\\284themes.txt")
names(listthemes) <- "theme"

library(stringr, lib.loc="D:\\R\\Lib")
str_detect(GDELT$themes, listthemes)
themesv2 <- str_split(GDELT$themesCharLoc, ";|,")
listthemes[[1]] == themesv2[[1]] #DEMOCRACY picked up but not ELECTION
